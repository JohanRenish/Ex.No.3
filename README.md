# Ex.No.3-Scenario-Based Report Development Utilizing Diverse Prompting Techniques for the the following Prompt Engineering types with examples - Straightforward Prompts - Tabular Format Prompting - Missing Word Prompting - Preceding Question Prompting.

### DATE:                                                                            
### REGISTER NUMBER : 
### Aim: To write the prompts for these following prompt types and evaluate that using any one method 1. Straightforward Prompts, 2. Tabular Format Prompting 3.Preceding Question Prompting and 4. Missing Word Prompting

### Explanation - Any one use case from Unit 5 and generate the report for that with the unit 2 Prompt type
Procedure:
1.	Straightforward Prompts:
    - •	"Define photosynthesis in one sentence."
    - 	"List three advantages of electric vehicles."

 3.	Tabular Format Prompting:
     •	"Compare and contrast AC and DC current in a table."
     •	"Provide a table listing five programming languages, their paradigms, and one use case each."
3. Preceding Question Prompting:
    •	"Why is climate change a global concern? Explain how greenhouse gases contribute to global warming."
    •	"How do vaccines work? Describe the process of immunization in simple terms."
4. Missing Word Prompting:
   •	"The capital of France is ____."
  •	"In photosynthesis, plants absorb sunlight to produce ____."


### Conclusion 


Result: The various types of Prompts are executed successfully with generated the report.



Scenario-Based Report: Exploring Prompting Techniques in Generative AI
Scenario Context
You are part of an AI research and training team working for an educational technology company, EduThink AI, which develops AI-driven learning tools for students and educators. Your task is to explore how different prompting strategies can improve the performance of a generative AI system when creating student-friendly study materials and analytical reports.
________________________________________
1. Comparative Analysis Prompt
Scenario
EduThink AI aims to evaluate how different AI models summarize academic content. The team uses a Comparative Analysis Prompt to analyze summaries generated by ChatGPT, Gemini, and Claude.
Example Prompt
“Compare the summarization styles of ChatGPT, Gemini, and Claude for a 500-word article on ‘The Basics of Blockchain Technology’. Analyze clarity, technical accuracy, and accessibility for undergraduate learners.”
Outcome
ChatGPT: Produced structured and pedagogical summaries.
Gemini: Focused on high-level abstraction, missing minor details.
Claude: Balanced technical and conceptual explanations.
Insight
Comparative prompts enhance evaluation skills of AI, promoting multi-model reasoning and cross-platform benchmarking — valuable for AI audit reports and education analytics.
________________________________________

2. Experiential Perspective Prompt
Scenario
The marketing department needs an AI-generated report reflecting human-like insight into student experience while using AI tutors.
Example Prompt
“From the perspective of a college student preparing for final exams, describe how an AI tutor powered by generative AI improves learning engagement and stress management.”
Outcome
The AI adopted a first-person narrative, integrating emotional and cognitive experiences, making the report empathetic and relatable.
Insight
Experiential prompts bridge AI outputs and human context, improving personalization in educational content creation.
________________________________________
3. Everyday Functioning Prompts
Scenario
The operations team wants AI assistance for daily task automation like scheduling, feedback analysis, and progress tracking.
Example Prompt
“Summarize today’s student feedback and list three key improvements for tomorrow’s class schedule.”
Outcome
AI generated quick actionable summaries, displaying context awareness and task adaptability.

Insight
Everyday functioning prompts enhance practical utility, embedding AI seamlessly into administrative and teaching routines.
________________________________________
4. Universal Prompt Structures
Scenario
EduThink wants consistent prompt formats usable across multiple departments (education, marketing, R&D).
Universal Prompt Framework
[Role] + [Task] + [Context] + [Output Format] + [Constraints]
Example
“As an educational analyst, summarize the key advantages of gamified learning in 100 words, focusing on student engagement.”
Outcome
Uniform, predictable responses with professional tone and format, regardless of task type.
Insight
Universal structures improve prompt reusability, scalability, and training efficiency in multi-department AI integration.
________________________________________
5. Prompt Refinements
Scenario
Initial AI outputs were too generic. The team refined prompts iteratively to increase depth and specificity.


Example

Initial Prompt:
“Explain blockchain simply.”
Refined Prompt:
“Explain blockchain technology to a 12th-grade commerce student using examples from digital payments.”
Outcome
Refined prompts led to context-sensitive, audience-appropriate outputs.
Insight
Prompt refinement enhances clarity, target accuracy, and domain alignment, especially for education and training applications.
________________________________________
6. Prompt Size Limitations
Scenario
When generating detailed research comparisons, AI models struggled with overly long prompts (exceeding token limits).
Example Issue
A 3,000-word input on “AI in healthcare” caused truncation and loss of focus.
Solution
The team used chunked prompts — dividing the input into sections (definition, application, ethics).
Insight
Managing prompt size ensures efficiency, memory optimization, and context retention during report generation.
________________________________________
7. Conclusion
Technique	Core Benefit	Use Case	Challenge
Comparative Analysis Prompt	Model benchmarking	Research evaluation	Requires structured metrics
Experiential Perspective Prompt	Human-like empathy	Storytelling, engagement	Subjectivity may vary
Everyday Functioning Prompt	Task automation	Daily reports, summaries	Needs frequent updates
Universal Prompt Structures	Standardization	Multi-team workflows	May reduce creativity
Prompt Refinement	Output accuracy	Educational content	Time-intensive iteration
Prompt Size Limitation	Performance stability	Long-form generation	Requires chunking strategy
________________________________________
Final Insight
Using a scenario-based prompting approach allows organizations like EduThink AI to create adaptive, efficient, and human-centered AI systems.
Combining structured, experiential, and functional prompts ensures that AI outputs are accurate, relatable, and operationally valuable, supporting both academic excellence and organizational productivity.






# Result: Thus the Prompts were exected succcessfully.

